# -*- mode: yaml; fill-column: 79-*-
#
# Example mrjob.conf file for running tilebrute
#
# Assumes you've installed tilebrute into a local virtualenv, ie
# /tmp/TILE_BRUTE.
#

runners:

  # run tilebrute on a distributed Hadoop install. Update 'python_bin' and
  # 'PYTHON' variables according to your local tilebrute installation.
  hadoop:
    bootstrap_mrjob: false
    python_bin: &local_python /tmp/TILE_BRUTE/bin/python
    cmdenv:
      PYTHON: *local_python
    hadoop_streaming_jar: target/tilebrute-0.1.0-SNAPSHOT.jar

  # run tilebrute on EMR. You'll need to update bucket locations to match your
  # own deployment topology. You can also change instance types, sizes, etc.
  # based on your interests.
  emr:
    # MRJob is installed on TaskTracers as part of restore_python.sh. Will not
    # shipping it via this config break master features?
    bootstrap_mrjob: false
    python_bin: &emr_python /home/hadoop/TILE_BRUTE-virtualenv-x86_64/bin/python
    cmdenv:
      # use python from virtualenv for tasks
      PYTHON: *emr_python
    hadoop_streaming_jar_on_emr: s3://tile-brute-us-west-2/emr_resources/tilebrute-0.1.0-SNAPSHOT.jar
    # check the output of tilebrute for a message like:
    #   "Connect to job tracker at: http://localhost:40144/jobtracker.jsp"
    ssh_tunnel_to_job_tracker: true
    # lock in on a specific, tested AMI/Hadoop version
    ami_version: '2.3'
    hadoop_version: '1.0.3'
    aws_region: us-west-2
    s3_log_uri: s3://tile-brute-us-west-2/logs/
    s3_scratch_uri: s3://tile-brute-us-west-2/tmp/mrjob/
    enable_emr_debugging: true

    #
    # Pipes in Hadoop config
    # 1 node, m1.large
    #
    # This configuration uses Hadoop to replicate the UNIX pipes runtime. It
    # runs on a single machine, executes each part of the program serially and
    # on a single core. That is, we configure the custer to run only a single
    # mapper (mapred.tasktracker.map.tasks.maximum=1) and a single reducer
    # (mapred.tasktracker.reduce.tasks.maximum=1). We configure the job to run
    # with only a single mapper (mapred.map.tasks=1) and a single reducer
    # (mapred.reduce.tasks=1), and start the reducer only after the mappers
    # have finished (mapred.reduce.slowstart.completed.maps=0.99).
    bootstrap_actions:
      - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
           -m mapred.map.tasks=1 \
           -m mapred.reduce.tasks=1 \
           -m mapred.reduce.slowstart.completed.maps=0.99 \
           -m mapred.tasktracker.map.tasks.maximum=1 \
           -m mapred.tasktracker.reduce.tasks.maximum=1"
      - s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh
    # only running 1 node, but we want spot, so specify a bid price.
    ec2_master_instance_type: m1.large # this is the cheapest x86_64 host
    ec2_master_instance_bid_price: 0.240 # bid at reserved price

    #
    # basic config
    # 1 node, m1.large
    #
    # This configuration is the first step up from UNIX pipes. Still running on
    # a single machine, but now using Hadoop for parallelism across cores. This
    # configuration will have no networking overhead, but it's still limited by
    # the capacity of the single machine. Force the job to run with 4 reducers
    # (reduce.tasks, reduce.tasks.maximum) to saturate this instance's cores.
    # Run job as a wave of mappers followed by a wave of reducers (slowstart).
    bootstrap_actions:
      - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
           -m mapred.map.tasks=1 \
           -m mapred.reduce.tasks=1 \
           -m mapred.reduce.slowstart.completed.maps=0.99 \
           -m mapred.tasktracker.map.tasks.maximum=1 \
           -m mapred.tasktracker.reduce.tasks.maximum=1"
      - s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh
    # only running 1 node, but we want spot, so specify a bid price.
    ec2_master_instance_type: m1.large # this is the cheapest x86_64 host
    ec2_master_instance_bid_price: 0.240 # bid at reserved price

    # #
    # # scaling up
    # # 1 node, c1.xlarge
    # #
    # # This configuration demonstrates taking single-node parallelism one step
    # # further. c1.xl sports 8 virtual cores with just as much ram as the
    # # m1.large. That's fine for us because generating tiles is not memory
    # # intensive. Again, do some hand-holding to take full advantage of all
    # # those cores.
    # bootstrap_actions:
    #   - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
    #        -m mapred.reduce.tasks=8 \
    #        -m mapred.tasktracker.reduce.tasks.maximum=8"
    #   - s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh
    # # only running 1 node, but we want spot, so specify a bid price.
    # ec2_master_instance_type: c1.xlarge # this is the cheapest x86_64 host
    # ec2_master_instance_bid_price: 0.580 # bid at reserved price

    # #
    # # baby's first cluster
    # # master: m1.small, core: 2x m1.large
    # #
    # # For this tiny cluster, try increasing the number of reduce slots per node
    # # -- our reduce isn't a heavy-weight process so we should gain significant
    # # speed boost from the extra parallelism.
    # bootstrap_actions:
    #   - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
    #        -m mapred.reduce.tasks=8 \
    #        -m mapred.reduce.slowstart.completed.maps=0.95 \
    #        -m mapred.tasktracker.map.tasks.maximum=4 \
    #        -m mapred.tasktracker.reduce.tasks.maximum=4"
    # # only install python on nodes that run tasks
    #   - "s3://elasticmapreduce/bootstrap-actions/run-if \
    #       instance.isRunningTaskTracker=true \
    #       s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh"
    # # EMR classifies instances into 3 different "roles": [MASTER, CORE, TASK].
    # # Master runs the JobTracker and NameNode. This can generally be a smallish
    # # instance type (MRJob defaults to m1.small) -- unless you plan to store
    # # lots of files, like tiles, on HDFS. EMR jobs are ephemeral clusters, they
    # # usually write through to S3, so this restriction doesn't apply for us.
    # ec2_instance_type: m1.large # this is the cheapest x86_64 host
    # ec2_core_instance_bid_price: 0.240 # bid at reserved price
    # ec2_master_instance_type: m1.small # use something cheap here
    # ec2_master_instance_bid_price: 0.060 # bid at reserved price
    # num_ec2_instances: 3

    # #
    # # demonstrate parallelism
    # # master: m1.small, core: 4x m1.large
    # #
    # # With 4x m1.large, there's now as many cores as the 1x c1.xlarge.
    # # Networking overhead and cluster orchestration not withstanding, we should
    # # hope to see comparable performance results. We still want to maximize
    # # throughput, so configure for a single "wave" of reducers.
    # bootstrap_actions:
    #   - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
    #        -m mapred.reduce.tasks=8 \
    #        -m mapred.reduce.slowstart.completed.maps=0.95 \
    #        -m mapred.tasktracker.map.tasks.maximum=4 \
    #        -m mapred.tasktracker.reduce.tasks.maximum=2"
    #   # only install python on nodes that run tasks
    #   - "s3://elasticmapreduce/bootstrap-actions/run-if \
    #       instance.isRunningTaskTracker=true \
    #       s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh"
    # ec2_instance_type: m1.large # this is the cheapest x86_64 host
    # ec2_core_instance_bid_price: 0.240 # bid at reserved price
    # ec2_master_instance_type: m1.small # use something cheap here
    # ec2_master_instance_bid_price: 0.060 # bid at reserved price
    # num_ec2_instances: 5

    # #
    # # compromise parallelism vs overhead
    # # master: m1.small, core: 4x c1.xlarge
    # #
    # # With 4x c1.xlarge, there's now far more cores applied. Hadoop
    # # orchestration overhead should be mitigated by the aggregate throughput.
    # bootstrap_actions:
    #   - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
    #        -m mapred.reduce.tasks=56 \
    #        -m mapred.reduce.slowstart.completed.maps=0.75 \
    #        -m mapred.tasktracker.reduce.tasks.maximum=8"
    #   # only install python on nodes that run tasks
    #   - "s3://elasticmapreduce/bootstrap-actions/run-if \
    #       instance.isRunningTaskTracker=true \
    #       s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh"
    # ec2_instance_type: c1.xlarge # this is the cheapest x86_64 host
    # ec2_core_instance_bid_price: 0.580 # bid at reserved price
    # ec2_master_instance_type: m1.small # use something cheap here
    # ec2_master_instance_bid_price: 0.060 # bid at reserved price
    # num_ec2_instances: 5

    # #
    # # Used to run the full country
    # # master: m1.large, core: 7x c1.xlarge, task: 12x c1.xlarge
    # #
    # # By default an EC2 account is capped at 20 concurrent hosts. This cluster
    # # consists of a moderate number of core nodes and is heavily supplemented
    # # with spot nodes. With a total compute size of 19 c1.xlarge, or 152 cores
    # # generating tiles in parallel, we're able to run through the entire
    # # dataset at zoom levels 4-17 inclusive in TODO:XXX hours.
    # #
    # # The cluster is configured for maximium reducer parallelism. We want to
    # # balance compeltion of "waves" of tasks in the face of unbalanced
    # # workloads. That is, some reduces will receive 5mm values while others
    # # will receive 50mm. We want the last wave of computations to finish at the
    # # same time so as to minimize idle cores (and thus, cost). The math is as
    # # follows:
    # #   8 reduce tasks/host * 19 hosts * 1.75 "waves" = 266 reduce tasks
    # #
    # # We also reduce the shuffle buffer size because some of the reducers
    # # receive more output than can be buffered. The alternative would be to use
    # # more reducers, perhaps 2.75 waves are appropriate for running over the
    # # entire country. See
    # # http://jamesmcminn.com/2012/10/fixing-hadoop-java-lang-outofmemoryerror-errors/
    # # for a more detailed description.
    # bootstrap_actions:
    #   - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
    #        -m mapred.reduce.tasks=266 \
    #        -m mapred.reduce.slowstart.completed.maps=0.75 \
    #        -m mapred.job.shuffle.input.buffer.percent=0.50 \
    #        -m mapred.tasktracker.reduce.tasks.maximum=8"
    #   # only install python on nodes that run tasks
    #   - "s3://elasticmapreduce/bootstrap-actions/run-if \
    #       instance.isRunningTaskTracker=true \
    #       s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh"
    # ec2_master_instance_type: m1.large # m1.large seems more stable than m1.small
    # ec2_core_instance_type: c1.xlarge  # on-demand worker nodes
    # ec2_task_instance_type: c1.xlarge  # spot worker nodes
    # ec2_task_instance_bid_price: 0.580 # bid at reserved price
    # num_ec2_core_instances: 7
    # num_ec2_task_instances: 12
