# -*- mode: yaml; fill-column: 79-*-
#
# Example mrjob.conf file for running tilebrute
#
# Assumes you've installed tilebrute into a local virtualenv, ie
# /tmp/TILE_BRUTE.
#

runners:

  # run tilebrute on a distributed Hadoop install. Update 'python_bin' and
  # 'PYTHON' variables according to your local tilebrute installation.
  hadoop:
    bootstrap_mrjob: false
    python_bin: &local_python /tmp/TILE_BRUTE/bin/python
    cmdenv:
      PYTHON: *local_python
    hadoop_streaming_jar: target/tile-brute-0.1.0-SNAPSHOT.jar

  # run tilebrute on EMR. You'll need to update bucket locations to match your
  # own deployment topology. You can also change instance types, sizes, etc.
  # based on your interests.
  emr:
    # MRJob is installed on TaskTracers as part of restore_python.sh. Will not
    # shipping it via this config break master features?
    bootstrap_mrjob: false
    python_bin: &emr_python /home/hadoop/TILE_BRUTE-virtualenv-x86_64/bin/python
    cmdenv:
      # use python from virtualenv for tasks
      PYTHON: *emr_python
    hadoop_streaming_jar_on_emr: s3://tile-brute-us-west-2/emr_resources/tile-brute-0.1.0-SNAPSHOT.jar
    # check the output of tilebrute for a message like:
    #   "Connect to job tracker at: http://localhost:40144/jobtracker.jsp"
    ssh_tunnel_to_job_tracker: true
    # lock in on a specific, tested AMI/Hadoop version
    ami_version: '2.3'
    hadoop_version: '1.0.3'
    aws_region: us-west-2
    s3_log_uri: s3://tile-brute-us-west-2/logs/
    s3_scratch_uri: s3://tile-brute-us-west-2/tmp/mrjob/

    #
    # basic config
    # 1 node, m1.large
    #
    # This configuration is the first step up from UNIX pipes. Still running on
    # a single machine, but now using Hadoop for parallelism across cores. This
    # configuration will have no networking overhead, but it's still limited by
    # the capacity of the single machine. Force the job to run with 4 reducers
    # (reduce.tasks, reduce.tasks.maximum) to saturate this instance's cores.
    # Run job as a wave of mappers followed by a wave of reducers (slowstart).
    bootstrap_actions:
      - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
           -m mapred.reduce.tasks=4 \
           -m mapred.reduce.slowstart.completed.maps=0.95 \
           -m mapred.tasktracker.reduce.tasks.maximum=4"
      - s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh
    # only running 1 node, but we want spot, so specify a bid price.
    ec2_master_instance_type: m1.large # this is the cheapest x86_64 host
    ec2_master_instance_bid_price: 0.240 # bid at reserved price

    # #
    # # scaling up
    # # 1 node, c1.xlarge
    # #
    # # This configuration demonstrates taking single-node parallelism one step
    # # further. c1.xl sports 8 virtual cores with just as much ram as the
    # # m1.large. That's fine for us because generating tiles is not memory
    # # intensive. Again, do some hand-holding to take full advantage of all
    # # those cores.
    # bootstrap_actions:
    #   - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
    #        -m mapred.reduce.tasks=8 \
    #        -m mapred.tasktracker.reduce.tasks.maximum=8"
    #   - s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh
    # # only running 1 node, but we want spot, so specify a bid price.
    # ec2_master_instance_type: c1.xlarge # this is the cheapest x86_64 host
    # ec2_master_instance_bid_price: 0.580 # bid at reserved price

    # #
    # # baby's first cluster
    # # master: m1.small, core: 2x m1.large
    # #
    # # For this tiny cluster, try increasing the number of reduce slots per node
    # # -- our reduce isn't a heavy-weight process so we should gain significant
    # # speed boost from the extra parallelism.
    # bootstrap_actions:
    #   - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
    #        -m mapred.reduce.tasks=8 \
    #        -m mapred.reduce.slowstart.completed.maps=0.95 \
    #        -m mapred.tasktracker.map.tasks.maximum=4 \
    #        -m mapred.tasktracker.reduce.tasks.maximum=4"
    # # only install python on nodes that run tasks
    #   - "s3://elasticmapreduce/bootstrap-actions/run-if \
    #       instance.isRunningTaskTracker=true \
    #       s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh"
    # # EMR classifies instances into 3 different "roles": [MASTER, CORE, TASK].
    # # Master runs the JobTracker and NameNode. This can generally be a smallish
    # # instance type (MRJob defaults to m1.small) -- unless you plan to store
    # # lots of files, like tiles, on HDFS. EMR jobs are ephemeral clusters, they
    # # usually write through to S3, so this restriction doesn't apply for us.
    # ec2_instance_type: m1.large # this is the cheapest x86_64 host
    # ec2_core_instance_bid_price: 0.240 # bid at reserved price
    # ec2_master_instance_type: m1.small # use something cheap here
    # ec2_master_instance_bid_price: 0.060 # bid at reserved price
    # num_ec2_instances: 3

    # #
    # # demonstrate parallelism
    # # master: m1.small, core: 4x m1.large
    # #
    # # With 4x m1.large, there's now as many cores as the 1x c1.xlarge.
    # # Networking overhead and cluster orchestration not withstanding, we should
    # # hope to see comparable performance results. We still want to maximize
    # # throughput, so configure for a single "wave" of reducers.
    # bootstrap_actions:
    #   - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
    #        -m mapred.reduce.tasks=8 \
    #        -m mapred.reduce.slowstart.completed.maps=0.95 \
    #        -m mapred.tasktracker.map.tasks.maximum=4 \
    #        -m mapred.tasktracker.reduce.tasks.maximum=2"
    #   # only install python on nodes that run tasks
    #   - "s3://elasticmapreduce/bootstrap-actions/run-if \
    #       instance.isRunningTaskTracker=true \
    #       s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh"
    # ec2_instance_type: m1.large # this is the cheapest x86_64 host
    # ec2_core_instance_bid_price: 0.240 # bid at reserved price
    # ec2_master_instance_type: m1.small # use something cheap here
    # ec2_master_instance_bid_price: 0.060 # bid at reserved price
    # num_ec2_instances: 5

    # #
    # # compromise parallelism vs overhead
    # # master: m1.small, core: 4x c1.xlarge
    # #
    # # With 4x c1.xlarge, there's now far more cores applied. Hadoop
    # # orchestration overhead should be mitigated by the aggregate throughput.
    # bootstrap_actions:
    #   - "s3://elasticmapreduce/bootstrap-actions/configure-hadoop \
    #        -m mapred.reduce.tasks=16 \
    #        -m mapred.reduce.slowstart.completed.maps=0.7"
    #   # only install python on nodes that run tasks
    #   - "s3://elasticmapreduce/bootstrap-actions/run-if \
    #       instance.isRunningTaskTracker=true \
    #       s3://tile-brute-us-west-2/emr_resources/bootstrap/restore_python.sh"
    # ec2_instance_type: c1.xlarge # this is the cheapest x86_64 host
    # ec2_core_instance_bid_price: 0.580 # bid at reserved price
    # ec2_master_instance_type: m1.large # use something cheap here
    # ec2_master_instance_bid_price: 0.240 # bid at reserved price
    # num_ec2_instances: 5
